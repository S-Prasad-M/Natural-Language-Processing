{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The doc object for Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x718b53aa7aa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in English\")\n",
    "type(introduction_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'English']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'VANISHING', 'GLASS', '\\n', 'N', 'early', 'ten', 'years', 'had', 'passed', 'since', 'the', 'Dursleys', 'had', 'woken', 'up', 'to', 'find', 'their', '\\n', 'nephew', 'on', 'the', 'front', 'step', ',', 'but', 'Privet', 'Drive', 'had', 'hardly', 'changed', 'at', 'all', '.', 'The', 'sun', '\\n', 'rose', 'on', 'the', 'same', 'tidy', 'front', 'gardens', 'and', 'lit', 'up', 'the', 'brass', 'number', 'four', 'on', 'the', '\\n', 'Dursleys', '’', 'front', 'door', ';', 'it', 'crept', 'into', 'their', 'living', 'room', ',', 'which', 'was', 'almost', 'exactly', '\\n', 'the', 'same', 'as', 'it', 'had', 'been', 'on', 'the', 'night', 'when', 'Mr.', 'Dursley', 'had', 'seen', 'that', 'fateful', 'news', '\\n', 'report', 'about', 'the', 'owls', '.', 'Only', 'the', 'photographs', 'on', 'the', 'mantelpiece', 'really', 'showed', '\\n', 'how', 'much', 'time', 'had', 'passed', '.', 'Ten', 'years', 'ago', ',', 'there', 'had', 'been', 'lots', 'of', 'pictures', 'of', '\\n', 'what', 'looked', 'like', 'a', 'large', 'pink', 'beach', 'ball', 'wearing', 'different', '-', 'colored', 'bonnets', '—', '\\n', 'but', 'Dudley', 'Dursley', 'was', 'no', 'longer', 'a', 'baby', ',', 'and', 'now', 'the', 'photographs', 'showed', 'a', '\\n', 'large', 'blond', 'boy', 'riding', 'his', 'first', 'bicycle', ',', 'on', 'a', 'carousel', 'at', 'the', 'fair', ',', 'playing', 'a', '\\n', 'computer', 'game', 'with', 'his', 'father', ',', 'being', 'hugged', 'and', 'kissed', 'by', 'his', 'mother', '.', 'The', '\\n', 'room', 'held', 'no', 'sign', 'at', 'all', 'that', 'another', 'boy', 'lived', 'in', 'the', 'house', ',', 'too', '.', '\\n', 'Yet', 'Harry', 'Potter', 'was', 'still', 'there', ',', 'asleep', 'at', 'the', 'moment', ',', 'but', 'not', 'for', 'long', '.', '\\n', 'His', 'Aunt', 'Petunia', 'was', 'awake', 'and', 'it', 'was', 'her', 'shrill', 'voice', 'that', 'made', 'the', 'first', 'noise', '\\n', 'of', 'the', 'day', '.', '\\n', '“', 'Up', '!', 'Get', 'up', '!', 'Now', '!', '”', '\\n', 'Harry', 'woke', 'with', 'a', 'start', '.', 'His', 'aunt', 'rapped', 'on', 'the', 'door', 'again', '.', '\\n', '“', 'Up', '!', '”', 'she', 'screeched', '.', 'Harry', 'heard', 'her', 'walking', 'toward', 'the', 'kitchen', 'and', '\\n', 'then', 'the', 'sound', 'of', 'the', 'frying', 'pan', 'being', 'put', 'on', 'the', 'stove', '.', 'He', 'rolled', 'onto', 'his', 'back', '\\n', 'and', 'tried', 'to', 'remember', 'the', 'dream', 'he', 'had', 'been', 'having', '.', 'It', 'had', 'been', 'a', 'good', 'one', '.', '\\n', 'There', 'had', 'been', 'a', 'flying', 'motorcycle', 'in', 'it', '.', 'He', 'had', 'a', 'funny', 'feeling', 'he', '’d', 'had', 'the', '\\n', 'same', 'dream', 'before', '.', '\\n', 'His', 'aunt', 'was', 'back', 'outside', 'the', 'door', '.', '\\n', '“', 'Are', 'you', 'up', 'yet', '?', '”', 'she', 'demanded', '.', '\\n', '“', 'Nearly', ',', '”', 'said', 'Harry', '.', '\\n', '“', 'Well', ',', 'get', 'a', 'move', 'on', ',', 'I', 'want', 'you', 'to', 'look', 'after', 'the', 'bacon', '.', 'And', 'do', 'n’t', 'you', '\\n', 'dare', 'let', 'it', 'burn', ',', 'I', 'want', 'everything', 'perfect', 'on', 'Duddy', '’s', 'birthday', '.', '”', '\\n', 'Harry', 'groaned', '.', '\\n', '“', 'What', 'did', 'you', 'say', '?', '”', 'his', 'aunt', 'snapped', 'through', 'the', 'door', '.', '\\n', '“', 'Nothing', ',', 'nothing', '…', '”', '\\n', 'Dudley', '’s', 'birthday', '—', 'how', 'could', 'he', 'have', 'forgotten', '?', 'Harry', 'got', 'slowly', 'out', '\\n', 'of', 'bed', 'and', 'started', 'looking', 'for', 'socks', '.', 'He', 'found', 'a', 'pair', 'under', 'his', 'bed', 'and', ',', 'after', '\\n', 'pulling', 'a', 'spider', 'off', 'one', 'of', 'them', ',', 'put', 'them', 'on', '.', 'Harry', 'was', 'used', 'to', 'spiders', ',', 'because', '\\n', 'the', 'cupboard', 'under', 'the', 'stairs', 'was', 'full', 'of', 'them', ',', 'and', 'that', 'was', 'where', 'he', 'slept', '.', '\\n', 'When', 'he', 'was', 'dressed', 'he', 'went', 'down', 'the', 'hall', 'into', 'the', 'kitchen', '.', 'The', 'table', '\\n', 'was', 'almost', 'hidden', 'beneath', 'all', 'Dudley', '’s', 'birthday', 'presents', '.', 'It', 'looked', 'as', 'though', '\\n', 'Dudley', 'had', 'gotten', 'the', 'new', 'computer', 'he', 'wanted', ',', 'not', 'to', 'mention', 'the', 'second', '\\n', 'television', 'and', 'the', 'racing', 'bike', '.', 'Exactly', 'why', 'Dudley', 'wanted', 'a', 'racing', 'bike', 'was', 'a', '\\n', 'mystery', 'to', 'Harry', ',', 'as', 'Dudley', 'was', 'very', 'fat', 'and', 'hated', 'exercise', '—', 'unless', 'of', 'course', '\\n', 'it', 'involved', 'punching', 'somebody', '.', 'Dudley', '’s', 'favorite', 'punching', 'bag', 'was', 'Harry', ',', 'but', '\\n', 'he', 'could', 'n’t', 'often', 'catch', 'him', '.', 'Harry', 'did', 'n’t', 'look', 'it', ',', 'but', 'he', 'was', 'very', 'fast', '.', '\\n', 'Perhaps', 'it', 'had', 'something', 'to', 'do', 'with', 'living', 'in', 'a', 'dark', 'cupboard', ',', 'but', 'Harry', '\\n', 'had', 'always', 'been', 'small', 'and', 'skinny', 'for', 'his', 'age', '.', 'He', 'looked', 'even', 'smaller', 'and', '\\n', 'skinnier', 'than', 'he', 'really', 'was', 'because', 'all', 'he', 'had', 'to', 'wear', 'were', 'old', 'clothes', 'of', '\\n', 'Dudley', '’s', ',', 'and', 'Dudley', 'was', 'about', 'four', 'times', 'bigger', 'than', 'he', 'was', '.', 'Harry', 'had', 'a', 'thin', '\\n', 'face', ',', 'knobbly', 'knees', ',', 'black', 'hair', ',', 'and', 'bright', 'green', 'eyes', '.', 'He', 'wore', 'round', 'glasses', '\\n', 'held', 'together', 'with', 'a', 'lot', 'of', 'Scotch', 'tape', 'because', 'of', 'all', 'the', 'times', 'Dudley', 'had', '\\n', 'punched', 'him', 'on', 'the', 'nose', '.', 'The', 'only', 'thing', 'Harry', 'liked', 'about', 'his', 'own', 'appearance', '\\n', 'was', 'a', 'very', 'thin', 'scar', 'on', 'his', 'forehead', 'that', 'was', 'shaped', 'like', 'a', 'bolt', 'of', 'lightning', '.', 'He', '\\n', 'had', 'had', 'it', 'as', 'long', 'as', 'he', 'could', 'remember', ',', 'and', 'the', 'first', 'question', 'he', 'could', 'ever', '\\n', 'remember', 'asking', 'his', 'Aunt', 'Petunia', 'was', 'how', 'he', 'had', 'gotten', 'it', '.', '\\n', '“', 'In', 'the', 'car', 'crash', 'when', 'your', 'parents', 'died', ',', '”', 'she', 'had', 'said', '.', '“', 'And', 'do', 'n’t', 'ask', '\\n', 'questions', '.', '”', '\\n', 'Do', 'n’t', 'ask', 'questions', '—', 'that', 'was', 'the', 'first', 'rule', 'for', 'a', 'quiet', 'life', 'with', 'the', '\\n', 'Dursleys', '.', '\\n', 'Uncle', 'Vernon', 'entered', 'the', 'kitchen', 'as', 'Harry', 'was', 'turning', 'over', 'the', 'bacon', '.', '\\n', '“', 'Comb', 'your', 'hair', '!', '”', 'he', 'barked', ',', 'by', 'way', 'of', 'a', 'morning', 'greeting', '.', '\\n', 'About', 'once', 'a', 'week', ',', 'Uncle', 'Vernon', 'looked', 'over', 'the', 'top', 'of', 'his', 'newspaper', '\\n', 'and', 'shouted', 'that', 'Harry', 'needed', 'a', 'haircut', '.', 'Harry', 'must', 'have', 'had', 'more', 'haircuts', 'than', '\\n', 'the', 'rest', 'of', 'the', 'boys', 'in', 'his', 'class', 'put', 'together', ',', 'but', 'it', 'made', 'no', 'difference', ',', 'his', 'hair', '\\n', 'simply', 'grew', 'that', 'way', '—', 'all', 'over', 'the', 'place', '.', '\\n', 'Harry', 'was', 'frying', 'eggs', 'by', 'the', 'time', 'Dudley', 'arrived', 'in', 'the', 'kitchen', 'with', 'his', '\\n', 'mother', '.', 'Dudley', 'looked', 'a', 'lot', 'like', 'Uncle', 'Vernon', '.', 'He', 'had', 'a', 'large', 'pink', 'face', ',', 'not', '\\n', 'much', 'neck', ',', 'small', ',', 'watery', 'blue', 'eyes', ',', 'and', 'thick', 'blond', 'hair', 'that', 'lay', 'smoothly', 'on', '\\n', 'his', 'thick', ',', 'fat', 'head', '.', 'Aunt', 'Petunia', 'often', 'said', 'that', 'Dudley', 'looked', 'like', 'a', 'baby', 'angel', '\\n', '—', 'Harry', 'often', 'said', 'that', 'Dudley', 'looked', 'like', 'a', 'pig', 'in', 'a', 'wig', '.', '\\n', 'Harry', 'put', 'the', 'plates', 'of', 'egg', 'and', 'bacon', 'on', 'the', 'table', ',', 'which', 'was', 'difficult', 'as', '\\n', 'there', 'was', 'n’t', 'much', 'room', '.', 'Dudley', ',', 'meanwhile', ',', 'was', 'counting', 'his', 'presents', '.', 'His', '\\n', 'face', 'fell', '.', '\\n', '“', 'Thirty', '-', 'six', ',', '”', 'he', 'said', ',', 'looking', 'up', 'at', 'his', 'mother', 'and', 'father', '.', '“', 'That', '’s', 'two', '\\n', 'less', 'than', 'last', 'year', '.', '”', '\\n', '“', 'Darling', ',', 'you', 'have', 'n’t', 'counted', 'Auntie', 'Marge', '’s', 'present', ',', 'see', ',', 'it', '’s', 'here', '\\n', 'under', 'this', 'big', 'one', 'from', 'Mummy', 'and', 'Daddy', '.', '”', '\\n', '“', 'All', 'right', ',', 'thirty', '-', 'seven', 'then', ',', '”', 'said', 'Dudley', ',', 'going', 'red', 'in', 'the', 'face', '.', 'Harry', ',', '\\n', 'who', 'could', 'see', 'a', 'huge', 'Dudley', 'tantrum', 'coming', 'on', ',', 'began', 'wolfing', 'down', 'his', 'bacon', '\\n', 'as', 'fast', 'as', 'possible', 'in', 'case', 'Dudley', 'turned', 'the', 'table', 'over', '.', '\\n', 'Aunt', 'Petunia', 'obviously', 'scented', 'danger', ',', 'too', ',', 'because', 'she', 'said', 'quickly', ',', '\\n', '“', 'And', 'we', '’ll', 'buy', 'you', 'another', 'two', 'presents', 'while', 'we', '’re', 'out', 'today', '.', 'How', '’s', 'that', ',', '\\n', 'popkin', '?', 'Two', 'more', 'presents', '.', 'Is', 'that', 'all', 'right', '”', '\\n', 'Dudley', 'thought', 'for', 'a', 'moment', '.', 'It', 'looked', 'like', 'hard', 'work', '.', 'Finally', 'he', 'said', '\\n', 'slowly', ',', '“', 'So', 'I', '’ll', 'have', 'thirty', '...', 'thirty', '...', '”', '\\n', '“', 'Thirty', '-', 'nine', ',', 'sweetums', ',', '”', 'said', 'Aunt', 'Petunia', '.', '\\n', '“', 'Oh', '.', '”', 'Dudley', 'sat', 'down', 'heavily', 'and', 'grabbed', 'the', 'nearest', 'parcel', '.', '“', 'All', 'right', '\\n', 'then', '.', '”', '\\n', 'Uncle', 'Vernon', 'chuckled', '.', '\\n', '“', 'Little', 'tyke', 'wants', 'his', 'money', '’s', 'worth', ',', 'just', 'like', 'his', 'father', '.', '’', 'Atta', 'boy', ',', '\\n', 'Dudley', '!', '”', 'He', 'ruffled', 'Dudley', '’s', 'hair', '.', '\\n', 'At', 'that', 'moment', 'the', 'telephone', 'rang', 'and', 'Aunt', 'Petunia', 'went', 'to', 'answer', 'it', '\\n', 'while', 'Harry', 'and', 'Uncle', 'Vernon', 'watched', 'Dudley', 'unwrap', 'the', 'racing', 'bike', ',', 'a', 'video', '\\n', 'camera', ',', 'a', 'remote', 'control', 'airplane', ',', 'sixteen', 'new', 'computer', 'games', ',', 'and', 'a', 'VCR', '.', 'He', '\\n', 'was', 'ripping', 'the', 'paper', 'off', 'a', 'gold', 'wristwatch', 'when', 'Aunt', 'Petunia', 'came', 'back', 'from', '\\n', 'the', 'telephone', 'looking', 'both', 'angry', 'and', 'worried', '.', '\\n', '“', 'Bad', 'news', ',', 'Vernon', ',', '”', 'she', 'said', '.', '“', 'Mrs.', 'Figg', '’s', 'broken', 'her', 'leg', '.', 'She', 'ca', 'n’t', '\\n', 'take', 'him', '.', '”', 'She', 'jerked', 'her', 'head', 'in', 'Harry', '’s', 'direction', '.', '\\n', 'Dudley', '’s', 'mouth', 'fell', 'open', 'in', 'horror', ',', 'but', 'Harry', '’s', 'heart', 'gave', 'a', 'leap', '.', 'Every', '\\n', 'year', 'on', 'Dudley', '’s', 'birthday', ',', 'his', 'parents', 'took', 'him', 'and', 'a', 'friend', 'out', 'for', 'the', 'day', ',', 'to', '\\n', 'adventure', 'parks', ',', 'hamburger', 'restaurants', ',', 'or', 'the', 'movies', '.', 'Every', 'year', ',', 'Harry', 'was', '\\n', 'left', 'behind', 'with', 'Mrs.', 'Figg', ',', 'a', 'mad', 'old', 'lady', 'who', 'lived', 'two', 'streets', 'away', '.', 'Harry', '\\n', 'hated', 'it', 'there', '.', 'The', 'whole', 'house', 'smelled', 'of', 'cabbage', 'and', 'Mrs.', 'Figg', 'made', 'him', '\\n', 'look', 'at', 'photographs', 'of', 'all', 'the', 'cats', 'she', '’d', 'ever', 'owned', '.', '\\n', '“', 'Now', 'what', '?', '”', 'said', 'Aunt', 'Petunia', ',', 'looking', 'furiously', 'at', 'Harry', 'as', 'though', '\\n', 'he', '’d', 'planned', 'this', '.', 'Harry', 'knew', 'he', 'ought', 'to', 'feel', 'sorry', 'that', 'Mrs.', 'Figg', 'had', 'broken', '\\n', 'her', 'leg', ',', 'but', 'it', 'was', 'n’t', 'easy', 'when', 'he', 'reminded', 'himself', 'it', 'would', 'be', 'a', 'whole', 'year', '\\n', 'before', 'he', 'had', 'to', 'look', 'at', 'Tibbles', ',', 'Snowy', ',', 'Mr.', 'Paws', ',', 'and', 'Tufty', 'again', '.', '\\n', '“', 'We', 'could', 'phone', 'Marge', ',', '”', 'Uncle', 'Vernon', 'suggested', '.', '\\n', '“', 'Do', 'n’t', 'be', 'silly', ',', 'Vernon', ',', 'she', 'hates', 'the', 'boy', '.', '”', '\\n', 'The', 'Dursleys', 'often', 'spoke', 'about', 'Harry', 'like', 'this', ',', 'as', 'though', 'he', 'was', 'n’t', 'there', '\\n', '—', 'or', 'rather', ',', 'as', 'though', 'he', 'was', 'something', 'very', 'nasty', 'that', 'could', 'n’t', 'understand', '\\n', 'them', ',', 'like', 'a', 'slug', '.', '\\n', '“', 'What', 'about', 'what’s', '-', 'her', '-', 'name', ',', 'your', 'friend', '—', 'Yvonne', '?', '”', '\\n', '“', 'On', 'vacation', 'in', 'Majorca', ',', '”', 'snapped', 'Aunt', 'Petunia', '.', '\\n', '“', 'You', 'could', 'just', 'leave', 'me', 'here', ',', '”', 'Harry', 'put', 'in', 'hopefully', '(', 'he', '’d', 'be', 'able', 'to', '\\n', 'watch', 'what', 'he', 'wanted', 'on', 'television', 'for', 'a', 'change', 'and', 'maybe', 'even', 'have', 'a', 'go', 'on', '\\n', 'Dudley', '’s', 'computer', ')', '.', '\\n', 'Aunt', 'Petunia', 'looked', 'as', 'though', 'she', '’d', 'just', 'swallowed', 'a', 'lemon', '.', '\\n', '“', 'And', 'come', 'back', 'and', 'find', 'the', 'house', 'in', 'ruins', '?', '”', 'she', 'snarled', '.', '\\n', '“', 'I', 'wo', 'n’t', 'blow', 'up', 'the', 'house', ',', '”', 'said', 'Harry', ',', 'but', 'they', 'were', 'n’t', 'listening', '.', '\\n', '“', 'I', 'suppose', 'we', 'could', 'take', 'him', 'to', 'the', 'zoo', ',', '”', 'said', 'Aunt', 'Petunia', 'slowly', ',', '“', '…', '\\n', 'and', 'leave', 'him', 'in', 'the', 'car', '.', '…', '”', '\\n', '“', 'That', 'car', '’s', 'new', ',', 'he', '’s', 'not', 'sitting', 'in', 'it', 'alone', '.', '…', '”', '\\n', 'Dudley', 'began', 'to', 'cry', 'loudly', '.', 'In', 'fact', ',', 'he', 'was', 'n’t', 'really', 'crying', '—', 'it', 'had', '\\n', 'been', 'years', 'since', 'he', '’d', 'really', 'cried', '—', 'but', 'he', 'knew', 'that', 'if', 'he', 'screwed', 'up', 'his', 'face', '\\n', 'and', 'wailed', ',', 'his', 'mother', 'would', 'give', 'him', 'anything', 'he', 'wanted', '.', '\\n', '“', 'Dinky', 'Duddydums', ',', 'do', 'n’t', 'cry', ',', 'Mummy', 'wo', 'n’t', 'let', 'him', 'spoil', 'your', 'special', '\\n', 'day', '!', '”', 'she', 'cried', ',', 'flinging', 'her', 'arms', 'around', 'him', '.', '\\n', '“', 'I', '…', 'don’t', '…', 'want', '…', 'him', '…', 't', '-', 't', '-', 'to', 'come', '!', '”', 'Dudley', 'yelled', 'between', 'huge', ',', '\\n', 'pretend', 'sobs', '.', '“', 'He', 'always', 'sp', '-', 'spoils', 'everything', '!', '”', 'He', 'shot', 'Harry', 'a', 'nasty', 'grin', '\\n', 'through', 'the', 'gap', 'in', 'his', 'mother', '’s', 'arms', '.', '\\n', 'Just', 'then', ',', 'the', 'doorbell', 'rang', '—', '“', 'Oh', ',', 'good', 'Lord', ',', 'they', '’re', 'here', '!', '”', 'said', 'Aunt', '\\n', 'Petunia', 'frantically', '—', 'and', 'a', 'moment', 'later', ',', 'Dudley', '’s', 'best', 'friend', ',', 'Piers', 'Polkiss', ',', '\\n', 'walked', 'in', 'with', 'his', 'mother', '.', 'Piers', 'was', 'a', 'scrawny', 'boy', 'with', 'a', 'face', 'like', 'a', 'rat', '.', 'He', '\\n', 'was', 'usually', 'the', 'one', 'who', 'held', 'people', '’s', 'arms', 'behind', 'their', 'backs', 'while', 'Dudley', 'hit', '\\n', 'them', '.', 'Dudley', 'stopped', 'pretending', 'to', 'cry', 'at', 'once', '.', '\\n', 'Half', 'an', 'hour', 'later', ',', 'Harry', ',', 'who', 'could', 'n’t', 'believe', 'his', 'luck', ',', 'was', 'sitting', 'in', 'the', '\\n', 'back', 'of', 'the', 'Dursleys', '’', 'car', 'with', 'Piers', 'and', 'Dudley', ',', 'on', 'the', 'way', 'to', 'the', 'zoo', 'for', 'the', '\\n', 'first', 'time', 'in', 'his', 'life', '.', 'His', 'aunt', 'and', 'uncle', 'had', 'n’t', 'been', 'able', 'to', 'think', 'of', 'anything', 'else', '\\n', 'to', 'do', 'with', 'him', ',', 'but', 'before', 'they', '’d', 'left', ',', 'Uncle', 'Vernon', 'had', 'taken', 'Harry', 'aside', '.', '\\n', '“', 'I', '’m', 'warning', 'you', ',', '”', 'he', 'had', 'said', ',', 'putting', 'his', 'large', 'purple', 'face', 'right', 'up', '\\n', 'close', 'to', 'Harry', '’s', ',', '“', 'I', '’m', 'warning', 'you', 'now', ',', 'boy', '—', 'any', 'funny', 'business', ',', 'anything', 'at', '\\n', 'all', '—', 'and', 'you', '’ll', 'be', 'in', 'that', 'cupboard', 'from', 'now', 'until', 'Christmas', '.', '”', '\\n', '“', 'I', '’m', 'not', 'going', 'to', 'do', 'anything', ',', '”', 'said', 'Harry', ',', '“', 'honestly', '…', '”', '\\n', 'But', 'Uncle', 'Vernon', 'did', 'n’t', 'believe', 'him', '.', 'No', 'one', 'ever', 'did', '.', '\\n', 'The', 'problem', 'was', ',', 'strange', 'things', 'often', 'happened', 'around', 'Harry', 'and', 'it', 'was', '\\n', 'just', 'no', 'good', 'telling', 'the', 'Dursleys', 'he', 'did', 'n’t', 'make', 'them', 'happen', '.', '\\n', 'Once', ',', 'Aunt', 'Petunia', ',', 'tired', 'of', 'Harry', 'coming', 'back', 'from', 'the', 'barbers', 'looking', '\\n', 'as', 'though', 'he', 'had', 'n’t', 'been', 'at', 'all', ',', 'had', 'taken', 'a', 'pair', 'of', 'kitchen', 'scissors', 'and', 'cut', 'his', '\\n', 'hair', 'so', 'short', 'he', 'was', 'almost', 'bald', 'except', 'for', 'his', 'bangs', ',', 'which', 'she', 'left', '“', 'to', 'hide', 'that', '\\n', 'horrible', 'scar', '.', '”', 'Dudley', 'had', 'laughed', 'himself', 'silly', 'at', 'Harry', ',', 'who', 'spent', 'a', 'sleepless', '\\n', 'night', 'imagining', 'school', 'the', 'next', 'day', ',', 'where', 'he', 'was', 'already', 'laughed', 'at', 'for', 'his', '\\n', 'baggy', 'clothes', 'and', 'taped', 'glasses', '.', 'Next', 'morning', ',', 'however', ',', 'he', 'had', 'gotten', 'up', 'to', '\\n', 'find', 'his', 'hair', 'exactly', 'as', 'it', 'had', 'been', 'before', 'Aunt', 'Petunia', 'had', 'sheared', 'it', 'off', 'He', 'had', '\\n', 'been', 'given', 'a', 'week', 'in', 'his', 'cupboard', 'for', 'this', ',', 'even', 'though', 'he', 'had', 'tried', 'to', 'explain', '\\n', 'that', 'he', 'could', 'n’t', 'explain', 'how', 'it', 'had', 'grown', 'back', 'so', 'quickly', '.', '\\n', 'Another', 'time', ',', 'Aunt', 'Petunia', 'had', 'been', 'trying', 'to', 'force', 'him', 'into', 'a', 'revolting', '\\n', 'old', 'sweater', 'of', 'Dudley', '’s', '(', 'brown', 'with', 'orange', 'puff', 'balls', ')', '.', 'The', 'harder', 'she', 'tried', 'to', '\\n', 'pull', 'it', 'over', 'his', 'head', ',', 'the', 'smaller', 'it', 'seemed', 'to', 'become', ',', 'until', 'finally', 'it', 'might', 'have', '\\n', 'fitted', 'a', 'hand', 'puppet', ',', 'but', 'certainly', 'would', 'n’t', 'fit', 'Harry', '.', 'Aunt', 'Petunia', 'had', 'decided', '\\n', 'it', 'must', 'have', 'shrunk', 'in', 'the', 'wash', 'and', ',', 'to', 'his', 'great', 'relief', ',', 'Harry', 'was', 'n’t', 'punished', '.', '\\n', 'On', 'the', 'other', 'hand', ',', 'he', '’d', 'gotten', 'into', 'terrible', 'trouble', 'for', 'being', 'found', 'on', 'the', '\\n', 'roof', 'of', 'the', 'school', 'kitchens', '.', 'Dudley', '’s', 'gang', 'had', 'been', 'chasing', 'him', 'as', 'usual', 'when', ',', '\\n', 'as', 'much', 'to', 'Harry', '’s', 'surprise', 'as', 'anyone', 'else', '’s', ',', 'there', 'he', 'was', 'sitting', 'on', 'the', '\\n', 'chimney', '.', 'The', 'Dursleys', 'had', 'received', 'a', 'very', 'angry', 'letter', 'from', 'Harry', '’s', '\\n', 'headmistress', 'telling', 'them', 'Harry', 'had', 'been', 'climbing', 'school', 'buildings', '.', 'But', 'all', 'he', '’d', '\\n', 'tried', 'to', 'do', '(', 'as', 'he', 'shouted', 'at', 'Uncle', 'Vernon', 'through', 'the', 'locked', 'door', 'of', 'his', '\\n', 'cupboard', ')', 'was', 'jump', 'behind', 'the', 'big', 'trash', 'cans', 'outside', 'the', 'kitchen', 'doors', '.', 'Harry', '\\n', 'supposed', 'that', 'the', 'wind', 'must', 'have', 'caught', 'him', 'in', 'mid', '-', 'jump', '.', '\\n', 'But', 'today', ',', 'nothing', 'was', 'going', 'to', 'go', 'wrong', '.', 'It', 'was', 'even', 'worth', 'being', 'with', '\\n', 'Dudley', 'and', 'Piers', 'to', 'be', 'spending', 'the', 'day', 'somewhere', 'that', 'was', 'n’t', 'school', ',', 'his', '\\n', 'cupboard', ',', 'or', 'Mrs.', 'Figg', '’s', 'cabbage', '-', 'smelling', 'living', 'room', '.', '\\n', 'While', 'he', 'drove', ',', 'Uncle', 'Vernon', 'complained', 'to', 'Aunt', 'Petunia', '.', 'He', 'liked', 'to', '\\n', 'complain', 'about', 'things', ':', 'people', 'at', 'work', ',', 'Harry', ',', 'the', 'council', ',', 'Harry', ',', 'the', 'bank', ',', 'and', '\\n', 'Harry', 'were', 'just', 'a', 'few', 'of', 'his', 'favorite', 'subjects', '.', 'This', 'morning', ',', 'it', 'was', 'motorcycles', '.', '\\n', '“', '…', 'roaring', 'along', 'like', 'maniacs', ',', 'the', 'young', 'hoodlums', ',', '”', 'he', 'said', ',', 'as', 'a', '\\n', 'motorcycle', 'overtook', 'them', '.', '\\n', '“', 'I', 'had', 'a', 'dream', 'about', 'a', 'motorcycle', ',', '”', 'said', 'Harry', ',', 'remembering', 'suddenly', '.', '\\n', '“', 'It', 'was', 'flying', '.', '”', '\\n', 'Uncle', 'Vernon', 'nearly', 'crashed', 'into', 'the', 'car', 'in', 'front', '.', 'He', 'turned', 'right', 'around', '\\n', 'in', 'his', 'seat', 'and', 'yelled', 'at', 'Harry', ',', 'his', 'face', 'like', 'a', 'gigantic', 'beet', 'with', 'a', 'mustache', ':', '\\n', '“', 'MOTORCYCLES', 'DON’T', 'FLY', '!', '”', '\\n', 'Dudley', 'and', 'Piers', 'sniggered', '.', '\\n', '“', 'I', 'know', 'they', 'do', 'n’t', ',', '”', 'said', 'Harry', '.', '“', 'It', 'was', 'only', 'a', 'dream', '.', '”', '\\n', 'But', 'he', 'wished', 'he', 'had', 'n’t', 'said', 'anything', '.', 'If', 'there', 'was', 'one', 'thing', 'the', '\\n', 'Dursleys', 'hated', 'even', 'more', 'than', 'his', 'asking', 'questions', ',', 'it', 'was', 'his', 'talking', 'about', '\\n', 'anything', 'acting', 'in', 'a', 'way', 'it', 'should', 'n’t', ',', 'no', 'matter', 'if', 'it', 'was', 'in', 'a', 'dream', 'or', 'even', 'a', '\\n', 'cartoon', '—', 'they', 'seemed', 'to', 'think', 'he', 'might', 'get', 'dangerous', 'ideas', '.', '\\n', 'It', 'was', 'a', 'very', 'sunny', 'Saturday', 'and', 'the', 'zoo', 'was', 'crowded', 'with', 'families', '.', 'The', '\\n', 'Dursleys', 'bought', 'Dudley', 'and', 'Piers', 'large', 'chocolate', 'ice', 'creams', 'at', 'the', 'entrance', 'and', '\\n', 'then', ',', 'because', 'the', 'smiling', 'lady', 'in', 'the', 'van', 'had', 'asked', 'Harry', 'what', 'he', 'wanted', 'before', '\\n', 'they', 'could', 'hurry', 'him', 'away', ',', 'they', 'bought', 'him', 'a', 'cheap', 'lemon', 'ice', 'pop', '.', 'It', 'was', 'n’t', '\\n', 'bad', ',', 'either', ',', 'Harry', 'thought', ',', 'licking', 'it', 'as', 'they', 'watched', 'a', 'gorilla', 'scratching', 'its', 'head', '\\n', 'who', 'looked', 'remarkably', 'like', 'Dudley', ',', 'except', 'that', 'it', 'was', 'n’t', 'blond', '.', '\\n', 'Harry', 'had', 'the', 'best', 'morning', 'he', '’d', 'had', 'in', 'a', 'long', 'time', '.', 'He', 'was', 'careful', 'to', '\\n', 'walk', 'a', 'little', 'way', 'apart', 'from', 'the', 'Dursleys', 'so', 'that', 'Dudley', 'and', 'Piers', ',', 'who', 'were', '\\n', 'starting', 'to', 'get', 'bored', 'with', 'the', 'animals', 'by', 'lunchtime', ',', 'would', 'n’t', 'fall', 'back', 'on', 'their', '\\n', 'favorite', 'hobby', 'of', 'hitting', 'him', '.', 'They', 'ate', 'in', 'the', 'zoo', 'restaurant', ',', 'and', 'when', 'Dudley', '\\n', 'had', 'a', 'tantrum', 'because', 'his', 'knickerbocker', 'glory', 'did', 'n’t', 'have', 'enough', 'ice', 'cream', 'on', '\\n', 'top', ',', 'Uncle', 'Vernon', 'bought', 'him', 'another', 'one', 'and', 'Harry', 'was', 'allowed', 'to', 'finish', 'the', '\\n', 'first', '.', '\\n', 'Harry', 'felt', ',', 'afterward', ',', 'that', 'he', 'should', 'have', 'known', 'it', 'was', 'all', 'too', 'good', 'to', '\\n', 'last', '.', '\\n', 'After', 'lunch', 'they', 'went', 'to', 'the', 'reptile', 'house', '.', 'It', 'was', 'cool', 'and', 'dark', 'in', 'there', ',', '\\n', 'with', 'lit', 'windows', 'all', 'along', 'the', 'walls', '.', 'Behind', 'the', 'glass', ',', 'all', 'sorts', 'of', 'lizards', 'and', '\\n', 'snakes', 'were', 'crawling', 'and', 'slithering', 'over', 'bits', 'of', 'wood', 'and', 'stone', '.', 'Dudley', 'and', '\\n', 'Piers', 'wanted', 'to', 'see', 'huge', ',', 'poisonous', 'cobras', 'and', 'thick', ',', 'man', '-', 'crushing', 'pythons', '.', '\\n', 'Dudley', 'quickly', 'found', 'the', 'largest', 'snake', 'in', 'the', 'place', '.', 'It', 'could', 'have', 'wrapped', 'its', '\\n', 'body', 'twice', 'around', 'Uncle', 'Vernon', '’s', 'car', 'and', 'crushed', 'it', 'into', 'a', 'trash', 'can', '—', 'but', 'at', '\\n', 'the', 'moment', 'it', 'did', 'n’t', 'look', 'in', 'the', 'mood', '.', 'In', 'fact', ',', 'it', 'was', 'fast', 'asleep', '.', '\\n', 'Dudley', 'stood', 'with', 'his', 'nose', 'pressed', 'against', 'the', 'glass', ',', 'staring', 'at', 'the', '\\n', 'glistening', 'brown', 'coils', '.', '\\n', '“', 'Make', 'it', 'move', ',', '”', 'he', 'whined', 'at', 'his', 'father', '.', 'Uncle', 'Vernon', 'tapped', 'on', 'the', '\\n', 'glass', ',', 'but', 'the', 'snake', 'did', 'n’t', 'budge', '.', '\\n', '“', 'Do', 'it', 'again', ',', '”', 'Dudley', 'ordered', '.', 'Uncle', 'Vernon', 'rapped', 'the', 'glass', 'smartly', '\\n', 'with', 'his', 'knuckles', ',', 'but', 'the', 'snake', 'just', 'snoozed', 'on', '.', '\\n', '“', 'This', 'is', 'boring', ',', '”', 'Dudley', 'moaned', '.', 'He', 'shuffled', 'away', '.', '\\n', 'Harry', 'moved', 'in', 'front', 'of', 'the', 'tank', 'and', 'looked', 'intently', 'at', 'the', 'snake', '.', 'He', '\\n', 'would', 'n’t', 'have', 'been', 'surprised', 'if', 'it', 'had', 'died', 'of', 'boredom', 'itself', '—', 'no', 'company', '\\n', 'except', 'stupid', 'people', 'drumming', 'their', 'fingers', 'on', 'the', 'glass', 'trying', 'to', 'disturb', 'it', 'all', '\\n', 'day', 'long', '.', 'It', 'was', 'worse', 'than', 'having', 'a', 'cupboard', 'as', 'a', 'bedroom', ',', 'where', 'the', 'only', '\\n', 'visitor', 'was', 'Aunt', 'Petunia', 'hammering', 'on', 'the', 'door', 'to', 'wake', 'you', 'up', ';', 'at', 'least', 'he', 'got', '\\n', 'to', 'visit', 'the', 'rest', 'of', 'the', 'house', '.', '\\n', 'The', 'snake', 'suddenly', 'opened', 'its', 'beady', 'eyes', '.', 'Slowly', ',', 'very', 'slowly', ',', 'it', 'raised', '\\n', 'its', 'head', 'until', 'its', 'eyes', 'were', 'on', 'a', 'level', 'with', 'Harry', '’s', '.', '\\n', 'It', 'winked', '.', '\\n', 'Harry', 'stared', '.', 'Then', 'he', 'looked', 'quickly', 'around', 'to', 'see', 'if', 'anyone', 'was', '\\n', 'watching', '.', 'They', 'were', 'n’t', '.', 'He', 'looked', 'back', 'at', 'the', 'snake', 'and', 'winked', ',', 'too', '.', '\\n', 'The', 'snake', 'jerked', 'its', 'head', 'toward', 'Uncle', 'Vernon', 'and', 'Dudley', ',', 'then', 'raised', '\\n', 'its', 'eyes', 'to', 'the', 'ceiling', '.', 'It', 'gave', 'Harry', 'a', 'look', 'that', 'said', 'quite', 'plainly', ':', '\\n', '“', 'I', 'get', 'that', 'all', 'the', 'time', '.', '”', '\\n', '“', 'I', 'know', ',', '”', 'Harry', 'murmured', 'through', 'the', 'glass', ',', 'though', 'he', 'was', 'n’t', 'sure', 'the', '\\n', 'snake', 'could', 'hear', 'him', '.', '“', 'It', 'must', 'be', 'really', 'annoying', '.', '”', '\\n', 'The', 'snake', 'nodded', 'vigorously', '.', '\\n', '“', 'Where', 'do', 'you', 'come', 'from', ',', 'anyway', '?', '”', 'Harry', 'asked', '.', '\\n', 'The', 'snake', 'jabbed', 'its', 'tail', 'at', 'a', 'little', 'sign', 'next', 'to', 'the', 'glass', '.', 'Harry', 'peered', 'at', '\\n', 'it', '.', '\\n', 'Boa', 'Constrictor', ',', 'Brazil', '.', '\\n', '“', 'Was', 'it', 'nice', 'there', '?', '”', '\\n', 'The', 'boa', 'constrictor', 'jabbed', 'its', 'tail', 'at', 'the', 'sign', 'again', 'and', 'Harry', 'read', 'on', ':', '\\n', 'This', 'specimen', 'was', 'bred', 'in', 'the', 'zoo', '.', '“', 'Oh', ',', 'I', 'see', '—', 'so', 'you', '’ve', 'never', 'been', 'to', '\\n', 'Brazil', '?', '”', '\\n', 'As', 'the', 'snake', 'shook', 'its', 'head', ',', 'a', 'deafening', 'shout', 'behind', 'Harry', 'made', 'both', '\\n', 'of', 'them', 'jump', '.', '“', 'DUDLEY', '!', 'MR', '.', 'DURSLEY', '!', 'COME', 'AND', 'LOOK', 'AT', 'THIS', '\\n', 'SNAKE', '!', 'YOU', 'WON’T', 'BELIEVE', 'WHAT', 'IT', '’S', 'DOING', '!', '”', '\\n', 'Dudley', 'came', 'waddling', 'toward', 'them', 'as', 'fast', 'as', 'he', 'could', '.', '\\n', '“', 'Out', 'of', 'the', 'way', ',', 'you', ',', '”', 'he', 'said', ',', 'punching', 'Harry', 'in', 'the', 'ribs', '.', 'Caught', 'by', '\\n', 'surprise', ',', 'Harry', 'fell', 'hard', 'on', 'the', 'concrete', 'floor', '.', 'What', 'came', 'next', 'happened', 'so', 'fast', '\\n', 'no', 'one', 'saw', 'how', 'it', 'happened', '—', 'one', 'second', ',', 'Piers', 'and', 'Dudley', 'were', 'leaning', 'right', '\\n', 'up', 'close', 'to', 'the', 'glass', ',', 'the', 'next', ',', 'they', 'had', 'leapt', 'back', 'with', 'howls', 'of', 'horror', '.', '\\n', 'Harry', 'sat', 'up', 'and', 'gasped', ';', 'the', 'glass', 'front', 'of', 'the', 'boa', 'constrictor', '’s', 'tank', 'had', '\\n', 'vanished', '.', 'The', 'great', 'snake', 'was', 'uncoiling', 'itself', 'rapidly', ',', 'slithering', 'out', 'onto', 'the', '\\n', 'floor', '.', 'People', 'throughout', 'the', 'reptile', 'house', 'screamed', 'and', 'started', 'running', 'for', 'the', '\\n', 'exits', '.', '\\n', 'As', 'the', 'snake', 'slid', 'swiftly', 'past', 'him', ',', 'Harry', 'could', 'have', 'sworn', 'a', 'low', ',', 'hissing', '\\n', 'voice', 'said', ',', '“', 'Brazil', ',', 'here', 'I', 'come', '.', '…', 'Thanksss', ',', 'amigo', '.', '”', '\\n', 'The', 'keeper', 'of', 'the', 'reptile', 'house', 'was', 'in', 'shock', '.', '\\n', '“', 'But', 'the', 'glass', ',', '”', 'he', 'kept', 'saying', ',', '“', 'where', 'did', 'the', 'glass', 'go', '?', '”', '\\n', 'The', 'zoo', 'director', 'himself', 'made', 'Aunt', 'Petunia', 'a', 'cup', 'of', 'strong', ',', 'sweet', 'tea', '\\n', 'while', 'he', 'apologized', 'over', 'and', 'over', 'again', '.', 'Piers', 'and', 'Dudley', 'could', 'only', 'gibber', '.', 'As', '\\n', 'far', 'as', 'Harry', 'had', 'seen', ',', 'the', 'snake', 'had', 'n’t', 'done', 'anything', 'except', 'snap', 'playfully', 'at', '\\n', 'their', 'heels', 'as', 'it', 'passed', ',', 'but', 'by', 'the', 'time', 'they', 'were', 'all', 'back', 'in', 'Uncle', 'Vernon', '’s', 'car', ',', '\\n', 'Dudley', 'was', 'telling', 'them', 'how', 'it', 'had', 'nearly', 'bitten', 'off', 'his', 'leg', ',', 'while', 'Piers', 'was', '\\n', 'swearing', 'it', 'had', 'tried', 'to', 'squeeze', 'him', 'to', 'death', '.', 'But', 'worst', 'of', 'all', ',', 'for', 'Harry', 'at', 'least', ',', '\\n', 'was', 'Piers', 'calming', 'down', 'enough', 'to', 'say', ',', '“', 'Harry', 'was', 'talking', 'to', 'it', ',', 'were', 'n’t', 'you', ',', '\\n', 'Harry', '?', '”', '\\n', 'Uncle', 'Vernon', 'waited', 'until', 'Piers', 'was', 'safely', 'out', 'of', 'the', 'house', 'before', '\\n', 'starting', 'on', 'Harry', '.', 'He', 'was', 'so', 'angry', 'he', 'could', 'hardly', 'speak', '.', 'He', 'managed', 'to', 'say', ',', '\\n', '“', 'Go', '—', 'cupboard', '—', 'stay', '—', 'no', 'meals', ',', '”', 'before', 'he', 'collapsed', 'into', 'a', 'chair', ',', 'and', '\\n', 'Aunt', 'Petunia', 'had', 'to', 'run', 'and', 'get', 'him', 'a', 'large', 'brandy', '.', '\\n', 'Harry', 'lay', 'in', 'his', 'dark', 'cupboard', 'much', 'later', ',', 'wishing', 'he', 'had', 'a', 'watch', '.', 'He', 'did', 'n’t', '\\n', 'know', 'what', 'time', 'it', 'was', 'and', 'he', 'could', 'n’t', 'be', 'sure', 'the', 'Dursleys', 'were', 'asleep', 'yet', '.', '\\n', 'Until', 'they', 'were', ',', 'he', 'could', 'n’t', 'risk', 'sneaking', 'to', 'the', 'kitchen', 'for', 'some', 'food', '.', '\\n', 'He', '’d', 'lived', 'with', 'the', 'Dursleys', 'almost', 'ten', 'years', ',', 'ten', 'miserable', 'years', ',', 'as', '\\n', 'long', 'as', 'he', 'could', 'remember', ',', 'ever', 'since', 'he', '’d', 'been', 'a', 'baby', 'and', 'his', 'parents', 'had', 'died', '\\n', 'in', 'that', 'car', 'crash', '.', 'He', 'could', 'n’t', 'remember', 'being', 'in', 'the', 'car', 'when', 'his', 'parents', 'had', '\\n', 'died', '.', 'Sometimes', ',', 'when', 'he', 'strained', 'his', 'memory', 'during', 'long', 'hours', 'in', 'his', '\\n', 'cupboard', ',', 'he', 'came', 'up', 'with', 'a', 'strange', 'vision', ':', 'a', 'blinding', 'flash', 'of', 'green', 'light', 'and', 'a', '\\n', 'burning', 'pain', 'on', 'his', 'forehead', '.', 'This', ',', 'he', 'supposed', ',', 'was', 'the', 'crash', ',', 'though', 'he', '\\n', 'could', 'n’t', 'imagine', 'where', 'all', 'the', 'green', 'light', 'came', 'from', '.', 'He', 'could', 'n’t', 'remember', 'his', '\\n', 'parents', 'at', 'all', '.', 'His', 'aunt', 'and', 'uncle', 'never', 'spoke', 'about', 'them', ',', 'and', 'of', 'course', 'he', 'was', '\\n', 'forbidden', 'to', 'ask', 'questions', '.', 'There', 'were', 'no', 'photographs', 'of', 'them', 'in', 'the', 'house', '.', '\\n', 'When', 'he', 'had', 'been', 'younger', ',', 'Harry', 'had', 'dreamed', 'and', 'dreamed', 'of', 'some', '\\n', 'unknown', 'relation', 'coming', 'to', 'take', 'him', 'away', ',', 'but', 'it', 'had', 'never', 'happened', ';', 'the', '\\n', 'Dursleys', 'were', 'his', 'only', 'family', '.', 'Yet', 'sometimes', 'he', 'thought', '(', 'or', 'maybe', 'hoped', ')', 'that', '\\n', 'strangers', 'in', 'the', 'street', 'seemed', 'to', 'know', 'him', '.', 'Very', 'strange', 'strangers', 'they', 'were', ',', '\\n', 'too', '.', 'A', 'tiny', 'man', 'in', 'a', 'violet', 'top', 'hat', 'had', 'bowed', 'to', 'him', 'once', 'while', 'out', 'shopping', '\\n', 'with', 'Aunt', 'Petunia', 'and', 'Dudley', '.', 'After', 'asking', 'Harry', 'furiously', 'if', 'he', 'knew', 'the', 'man', ',', '\\n', 'Aunt', 'Petunia', 'had', 'rushed', 'them', 'out', 'of', 'the', 'shop', 'without', 'buying', 'anything', '.', 'A', 'wild-', '\\n', 'looking', 'old', 'woman', 'dressed', 'all', 'in', 'green', 'had', 'waved', 'merrily', 'at', 'him', 'once', 'on', 'a', 'bus', '.', '\\n', 'A', 'bald', 'man', 'in', 'a', 'very', 'long', 'purple', 'coat', 'had', 'actually', 'shaken', 'his', 'hand', 'in', 'the', 'street', '\\n', 'the', 'other', 'day', 'and', 'then', 'walked', 'away', 'without', 'a', 'word', '.', 'The', 'weirdest', 'thing', 'about', 'all', '\\n', 'these', 'people', 'was', 'the', 'way', 'they', 'seemed', 'to', 'vanish', 'the', 'second', 'Harry', 'tried', 'to', 'get', 'a', '\\n', 'closer', 'look', '.', '\\n', 'At', 'school', ',', 'Harry', 'had', 'no', 'one', '.', 'Everybody', 'knew', 'that', 'Dudley', '’s', 'gang', 'hated', '\\n', 'that', 'odd', 'Harry', 'Potter', 'in', 'his', 'baggy', 'old', 'clothes', 'and', 'broken', 'glasses', ',', 'and', 'nobody', '\\n', 'liked', 'to', 'disagree', 'with', 'Dudley', '’s', 'gang', '.']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "file_name = \"example.txt\"\n",
    "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
    "print([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "...    \"Gus Proto is a python developer currently\"\n",
    "...    \" working for a London-based Fintech\"\n",
    "...    \" company. He is interested in learning\"\n",
    "...    \" Natural Language Processing.\"\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a python...\n",
      "He is interested in learning...\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"{sentence[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsis_text = (\n",
    "    \"Gus, can you, ... never mind, I forgot\"\n",
    "    \" what I was saying. So, do you think\"\n",
    "    \" we should ...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'{\"Text with Whitespace\":22}'\n",
    "    f'{\"Is Alphanumeric?\":15}'\n",
    "    f'{\"Is Punctuation?\":18}'\n",
    "    f'{\"Is Stop Word?\"}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus                   True           False             False\n",
      "Proto                 True           False             False\n",
      "is                    True           False             True\n",
      "a                     True           False             True\n",
      "Python                True           False             False\n",
      "developer             True           False             False\n",
      "currently             True           False             False\n",
      "working               True           False             False\n",
      "for                   True           False             True\n",
      "a                     True           False             True\n",
      "London                True           False             False\n",
      "-                     False          True              False\n",
      "based                 True           False             False\n",
      "Fintech               True           False             False\n",
      "company               True           False             False\n",
      ".                     False          True              False\n",
      "He                    True           False             True\n",
      "is                    True           False             True\n",
      "interested            True           False             False\n",
      "in                    True           False             True\n",
      "learning              True           False             False\n",
      "Natural               True           False             False\n",
      "Language              True           False             False\n",
      "Processing            True           False             False\n",
      ".                     False          True              False\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print(\n",
    "        f'{str(token.text_with_ws):22}'\n",
    "        f'{str(token.is_alpha):15}'\n",
    "        f'{str(token.is_punct):18}'\n",
    "        f'{str(token.is_stop)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '-', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "tokens_to_print = [token.text for token in about_doc[8:15]]\n",
    "print(tokens_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "\n",
    "custom_infixes = [r\"@\"]\n",
    "infix_re = spacy.util.compile_infix_regex(\n",
    "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    ")\n",
    "\n",
    "custom_nlp.tokenizer = Tokenizer(\n",
    "    custom_nlp.vocab,\n",
    "    prefix_search=prefix_re.search,\n",
    "    suffix_search=suffix_re.search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=None,\n",
    ")\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    "\n",
    ")\n",
    "\n",
    "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "\n",
    "print([token.text for token in custom_tokenizer_about_doc[8:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether\n",
      "thus\n",
      "used\n",
      "latterly\n",
      "via\n",
      "beside\n",
      "myself\n",
      "due\n",
      "noone\n",
      "using\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_stopwords = spacy. lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords) [:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'Python', 'developer', 'currently', 'working', 'London', '-', 'based', 'Fintech', 'company', '.', 'interested', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "about_doc = nlp(custom_about_text)\n",
    "\n",
    "filtered_tokens = [token.text for token in about_doc if not token.is_stop]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is: be\n",
      "          Processing: processing\n",
      "                  He: he\n",
      "               keeps: keep\n",
      "          organizing: organize\n",
      "             meetups: meetup\n",
      "               talks: talk\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "conference_help_text = (\n",
    "    \"Gus is helping organize a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organizing local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    ")\n",
    "\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "\n",
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20}: {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token. is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\n",
    "    [token.text for token in complete_doc if not token. is_punct]\n",
    ").most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Proto\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: Python\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: developer\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: currently\n",
      "=====\n",
      "TAG: RB         POS: ADV\n",
      "EXPLANATION: adverb\n",
      "\n",
      "TOKEN: working\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: London\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: -\n",
      "=====\n",
      "TAG: HYPH       POS: PUNCT\n",
      "EXPLANATION: punctuation mark, hyphen\n",
      "\n",
      "TOKEN: based\n",
      "=====\n",
      "TAG: VBN        POS: VERB\n",
      "EXPLANATION: verb, past participle\n",
      "\n",
      "TOKEN: Fintech\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: company\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "=====\n",
      "TAG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: interested\n",
      "=====\n",
      "TAG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: in\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: Natural\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Language\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Processing\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {str(token)}\n",
    "=====\n",
    "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for token in about_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[developer, company]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[interested]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize using displaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"51d36136a0fb4cfb8ae8eb34d6c35c1c-0\" class=\"displacy\" width=\"850\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Processing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-0\" stroke-width=\"2px\" d=\"M70,102.0 C70,52.0 145.0,52.0 145.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,104.0 L62,92.0 78,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-1\" stroke-width=\"2px\" d=\"M170,102.0 C170,52.0 245.0,52.0 245.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245.0,104.0 L253.0,92.0 237.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-2\" stroke-width=\"2px\" d=\"M270,102.0 C270,52.0 345.0,52.0 345.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M345.0,104.0 L353.0,92.0 337.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-3\" stroke-width=\"2px\" d=\"M370,102.0 C370,52.0 445.0,52.0 445.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,104.0 L453.0,92.0 437.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-4\" stroke-width=\"2px\" d=\"M570,102.0 C570,52.0 645.0,52.0 645.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,104.0 L562,92.0 578,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-5\" stroke-width=\"2px\" d=\"M670,102.0 C670,52.0 745.0,52.0 745.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,104.0 L662,92.0 678,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-6\" stroke-width=\"2px\" d=\"M470,102.0 C470,2.0 750.0,2.0 750.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-51d36136a0fb4cfb8ae8eb34d6c35c1c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,104.0 L758.0,92.0 742.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_interest_text = \"He is interested in learning Natural Language Processing\"\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "\n",
    "html_code = displacy.render(about_interest_doc, style=\"dep\", options={\"distance\":100})\n",
    "print(html_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gus', 'proto', 'python', 'developer', 'currently', 'work', 'london', 'base', 'fintech', 'company', 'interested', 'learn', 'natural', 'language', 'processing', 'developer', 'conference', 'happen', '21', 'july', '2019', 'london', 'title', 'application', 'natural', 'language', 'processing', 'helpline', 'numberavailable', '+44', '1234567891', 'gus', 'helping', 'organize', 'keep', 'organize', 'local', 'python', 'meetup', 'internal', 'talk', 'workplace', 'gus', 'present', 'talk', 'talk', 'introduce', 'reader', 'use', 'case', 'natural', 'language', 'processing', 'fintech\"', 'apart', 'work', 'passionate', 'music', 'gus', 'learn', 'play', 'piano', 'enrol', 'weekend', 'batch', 'great', 'piano', 'academy', 'great', 'piano', 'academy', 'situate', 'mayfair', 'city', 'london', 'world', 'class', 'piano', 'instructor']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \"available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \"Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    ")\n",
    "\n",
    "def preprocess_token(token):\n",
    "    return token. lemma_. strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in complete_doc\n",
    "    if is_token_allowed(token)\n",
    "]\n",
    "print(complete_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based matching spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\" \n",
    ")\n",
    "\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"} ]\n",
    "    matcher.add(\"FULL_NAME\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        yield span.text\n",
    "\n",
    "print(next(extract_full_name(about_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "\n",
      "token.tag_ ='NNP'\n",
      "token.head.text ='learning'\n",
      "token. dep_ ='nsubj'\n",
      "\n",
      "TOKEN: is\n",
      "\n",
      "token.tag_ ='VBZ'\n",
      "token.head.text ='learning'\n",
      "token. dep_ ='aux'\n",
      "\n",
      "TOKEN: learning\n",
      "\n",
      "token.tag_ ='VBG'\n",
      "token.head.text ='learning'\n",
      "token. dep_ ='ROOT'\n",
      "\n",
      "TOKEN: piano\n",
      "\n",
      "token.tag_ ='NN'\n",
      "token.head.text ='learning'\n",
      "token. dep_ ='dobj'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "piano_text = \"Gus is learning piano\"\n",
    "piano_doc = nlp(piano_text)\n",
    "\n",
    "for token in piano_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "\n",
    "{token.tag_ =}\n",
    "{token.head.text =}\n",
    "{token. dep_ =}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6bbcef9497074f1082552f7a4a75ba9f-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Gus</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">piano</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6bbcef9497074f1082552f7a4a75ba9f-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6bbcef9497074f1082552f7a4a75ba9f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6bbcef9497074f1082552f7a4a75ba9f-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6bbcef9497074f1082552f7a4a75ba9f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6bbcef9497074f1082552f7a4a75ba9f-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6bbcef9497074f1082552f7a4a75ba9f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Python', 'working']\n",
      "Python\n",
      "currently\n",
      "['a', 'Python']\n",
      "['working']\n",
      "[a, Python, developer, currently, working, for, a, London, -, based, Fintech, company]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "\n",
    "one_line_about_text = (\n",
    "    \"Gus Proto is a Python developer\"\n",
    "    \" currently working for a London-based Fintech company\"\n",
    ")\n",
    "\n",
    "one_line_about_doc = nlp(one_line_about_text)\n",
    "print([token.text for token in one_line_about_doc[5].children])\n",
    "print(one_line_about_doc[5].nbor(-1))\n",
    "print(one_line_about_doc[5].nbor())\n",
    "print([token.text for token in one_line_about_doc[5].lefts])\n",
    "print([token.text for token in one_line_about_doc[5].rights])\n",
    "print(list(one_line_about_doc[5].subtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recogniition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ent.text ='Great Piano Academy'\n",
      "ent.start_char =0\n",
      "ent.end_char =19\n",
      "ent.label_ ='ORG'\n",
      "spacy.explain('ORG') = Companies, agencies, institutions, etc.\n",
      "\n",
      "ent.text ='Mayfair'\n",
      "ent.start_char =35\n",
      "ent.end_char =42\n",
      "ent.label_ ='FAC'\n",
      "spacy.explain('FAC') = Buildings, airports, highways, bridges, etc.\n",
      "\n",
      "ent.text ='the City of London'\n",
      "ent.start_char =46\n",
      "ent.end_char =64\n",
      "ent.label_ ='GPE'\n",
      "spacy.explain('GPE') = Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "piano_class_text = (\n",
    "    \"Great Piano Academy is situated\"\n",
    "    \" in Mayfair or the City of London and has\"\n",
    "    \" world-class piano instructors.\"\n",
    ")\n",
    "\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "{ent.text =}\n",
    "{ent.start_char =}\n",
    "{ent.end_char =}\n",
    "{ent.label_ =}\n",
    "spacy.explain('{ent.label_}') = {spacy.explain(ent.label_)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Piano Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Dec/2024 08:54:38] \"GET / HTTP/1.1\" 200 1425\n",
      "127.0.0.1 - - [13/Dec/2024 08:54:39] \"GET /favicon.ico HTTP/1.1\" 200 1425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_class_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5 people surveyed, [REDACTED] , [REDACTED] and [REDACTED] like apples. [REDACTED] and [REDACTED] like oranges.\n"
     ]
    }
   ],
   "source": [
    "survey_text = (\n",
    "    \"Out of 5 people surveyed, James Robert,\"\n",
    "    \" Julie Fuller and Benjamin Brooks like\"\n",
    "    \" apples. Kelly Cox and Matthew Evans\"\n",
    "    \" like oranges.\"\n",
    ")\n",
    "\n",
    "def replace_person_names(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    return token.text_with_ws\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    with nlp_doc.retokenize() as retokenizer:\n",
    "        for ent in nlp_doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(replace_person_names, nlp_doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "survey_doc = nlp(survey_text)\n",
    "print(redact_names(survey_doc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
