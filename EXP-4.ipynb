{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> NLP Lab 4 </h1>\n",
    "\n",
    "****\n",
    "NAME: SHREE PRASAD M   \n",
    "ROLL NUMBER: CH.EN.U4AIE22050\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:47:03.732093: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:47:03.735818: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:47:03.744979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734688023.759392   17280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734688023.763727   17280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 09:47:03.787599: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
      "\n",
      "All the possible Bigrams are\n",
      "[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n",
      "\n",
      "Bigrams along with their frequency\n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
      "\n",
      "Unigrams along with their frequency\n",
      "{'This': 3, 'is': 3, 'a': 2, 'I': 1, 'love': 1, 'my': 2}\n",
      "\n",
      "Bigrams along with their probability\n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n",
      "\n",
      "The bigrams in the given sentence are\n",
      "[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n"
     ]
    }
   ],
   "source": [
    "def readData():\n",
    "    data = ['This is a dog', 'This is a cat', 'I love my cat', 'This is my name']\n",
    "    dat = []\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "    listOfBigrams = []\n",
    "    bigramCounts = {}\n",
    "    unigramCounts = {}\n",
    "    for i in range(len(data) - 1):\n",
    "        if i < len(data) - 1 and data[i + 1].islower():\n",
    "            listOfBigrams.append((data[i], data[i + 1]))\n",
    "            if (data[i], data[i + 1]) in bigramCounts:\n",
    "                bigramCounts[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                bigramCounts[(data[i], data[i + 1])] = 1\n",
    "            if data[i] in unigramCounts:\n",
    "                unigramCounts[data[i]] += 1\n",
    "            else:\n",
    "                unigramCounts[data[i]] = 1\n",
    "    return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram)) / (unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "    print(\"\\nAll the possible Bigrams are\")\n",
    "    print(listOfBigrams)\n",
    "\n",
    "    print(\"\\nBigrams along with their frequency\")\n",
    "    print(bigramCounts)\n",
    "\n",
    "    print(\"\\nUnigrams along with their frequency\")\n",
    "    print(unigramCounts)\n",
    "\n",
    "    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "    print(\"\\nBigrams along with their probability\")\n",
    "    print(bigramProb)\n",
    "    \n",
    "    inputList = \"This is my cat\"\n",
    "    splt = inputList.split()\n",
    "    outputProb1 = 1\n",
    "    bilist = []\n",
    "\n",
    "    for i in range(len(splt) - 1):\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "    print(\"\\nThe bigrams in the given sentence are\")\n",
    "    print(bilist)\n",
    "    \n",
    "    for i in range(len(bilist)):\n",
    "        if bilist[i] in bigramProb:\n",
    "            outputProb1 *= bigramProb[bilist[i]]\n",
    "        else:\n",
    "            outputProb1 *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'public': 0.05555555555555555,\n",
       " 'European': 0.05555555555555555,\n",
       " 'Bank': 0.05555555555555555,\n",
       " 'price': 0.1111111111111111,\n",
       " 'emirate': 0.05555555555555555,\n",
       " 'overseas': 0.05555555555555555,\n",
       " 'newspaper': 0.05555555555555555,\n",
       " 'company': 0.16666666666666666,\n",
       " 'Turkish': 0.05555555555555555,\n",
       " 'increase': 0.05555555555555555,\n",
       " 'options': 0.05555555555555555,\n",
       " 'Higher': 0.05555555555555555,\n",
       " 'pound': 0.05555555555555555,\n",
       " 'Italian': 0.05555555555555555,\n",
       " 'time': 0.05555555555555555}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurrence\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "dict(model[\"today\", \"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the price of 12 , 000 , respectively , for example , on sale of 7 , 446 , 958 , 423 vs 2 , 723 Sales 10 . 9 IN 1985 / 86 .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Starting words\n",
    "text = [\"today\", \"the\"]\n",
    "sentence_finished = False\n",
    "\n",
    "while not sentence_finished:\n",
    "    # Select a random probability threshold\n",
    "    r = random.random()\n",
    "    accumulator = 0.0\n",
    "\n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        accumulator += model[tuple(text[-2:])][word]\n",
    "        # Select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "# Join and print the generated text\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 13:06:54.145554: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-31 13:06:54.368948: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-31 13:06:54.561256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735650414.784253    3083 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735650414.925484    3083 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 13:06:55.554889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import re\n",
    "\n",
    "data_text = \"\"\"The unanimous Declaration of the thirteen united States of America, When in\n",
    "We hold these truths to be self-evident, that all men are created equal, that they are endou\n",
    "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
    "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless su:\n",
    "He has refused to pass other Laws for the accommodation of large districts of people, unles:\n",
    "He has called together legislative bodies at places unusual, uncomfortable, and distant fror\n",
    "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his inv\n",
    "He has refused for a long time, after such dissolutions, to cause others to be elected; whel\n",
    "He has endeavoured to prevent the population of these States; for that purpose obstructing 1\n",
    "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establi:\n",
    "He has made Judges dependent on his Will alone, for the tenure of their offices, and the amc\n",
    "He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass oul\n",
    "He has kept among us, in times of peace, Standing Armies without the Consent of our legislat\n",
    "He has affected to render the Military independent of and superior to the Civil power.\n",
    "He has combined with others to subject us to a jurisdiction foreign to our constitution, an\n",
    "For Quartering large bodies of armed troops among us:\n",
    "For protecting them, by a mock Trial, from punishment for any Murders which they should comr\n",
    "For cutting off our Trade with all parts of the world:\n",
    "For imposing Taxes on us without our Consent:\n",
    "For depriving us in many cases, of the benefits of Trial by Jury:\n",
    "For transporting us beyond Seas to be tried for pretended offences\n",
    "For abolishing the free System of English Laws in a neighbouring Province, establishing thel\n",
    "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally\n",
    "For suspending our own Legislatures, and declaring themselves invested with power to legisl\n",
    "He has abdicated Government here, by declaring us out of his Protection and waging War agair\n",
    "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of (\n",
    "He is at this time transporting large Armies of foreign Mercenaries to compleat the works o\n",
    "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against 1\n",
    "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhab:\n",
    "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms\n",
    "Nor have We been wanting in attentions to our Brittish brethren. We have warned them from t:\n",
    "We, therefore, the Representatives of the united States of America, in General Congress, As:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r\"'s\\b\", \"\", newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    long_words = []\n",
    "    for i in newString.split():\n",
    "        if len(i) >= 3:\n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "data_new = text_cleaner(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 2366\n"
     ]
    }
   ],
   "source": [
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        seq = text[i-length:i+1]\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences\n",
    "sequences = create_seq(data_new)\n",
    "# Total Sequences: 7052\n",
    "\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq, mapping):\n",
    "    encoded_seqs = []\n",
    "    for line in seq:\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        encoded_seqs.append(encoded_seq)\n",
    "    return np.array(encoded_seqs)\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  8,  5, ...,  5,  0, 20],\n",
       "       [ 8,  5,  0, ...,  0, 20,  8],\n",
       "       [ 5,  0, 21, ..., 20,  8,  9],\n",
       "       ...,\n",
       "       [ 4,  0, 19, ...,  7, 18,  5],\n",
       "       [ 0, 19, 20, ..., 18,  5, 19],\n",
       "       [19, 20,  1, ...,  5, 19, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2129, 30) Val shape: (237, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2129, 30) Val shape: (237, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Reshape sequences to make it 2-dimensional\n",
    "sequences = sequences.reshape((len(sequences), sequences.shape[1]))\n",
    "\n",
    "# create X and y\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-31 13:08:20.133255: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "67/67 - 4s - 65ms/step - acc: 0.1348 - loss: 2.9806 - val_acc: 0.1772 - val_loss: 2.8968\n",
      "Epoch 2/100\n",
      "67/67 - 2s - 30ms/step - acc: 0.1982 - loss: 2.7097 - val_acc: 0.2700 - val_loss: 2.5975\n",
      "Epoch 3/100\n",
      "67/67 - 2s - 29ms/step - acc: 0.2654 - loss: 2.4433 - val_acc: 0.2658 - val_loss: 2.4505\n",
      "Epoch 4/100\n",
      "67/67 - 2s - 34ms/step - acc: 0.2950 - loss: 2.3215 - val_acc: 0.2785 - val_loss: 2.3988\n",
      "Epoch 5/100\n",
      "67/67 - 3s - 42ms/step - acc: 0.3288 - loss: 2.2533 - val_acc: 0.2911 - val_loss: 2.3669\n",
      "Epoch 6/100\n",
      "67/67 - 2s - 33ms/step - acc: 0.3410 - loss: 2.1939 - val_acc: 0.3038 - val_loss: 2.3138\n",
      "Epoch 7/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.3481 - loss: 2.1387 - val_acc: 0.3418 - val_loss: 2.2771\n",
      "Epoch 8/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.3645 - loss: 2.0856 - val_acc: 0.3291 - val_loss: 2.2599\n",
      "Epoch 9/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.3805 - loss: 2.0299 - val_acc: 0.3249 - val_loss: 2.2328\n",
      "Epoch 10/100\n",
      "67/67 - 2s - 37ms/step - acc: 0.3894 - loss: 1.9849 - val_acc: 0.3207 - val_loss: 2.2197\n",
      "Epoch 11/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.3964 - loss: 1.9358 - val_acc: 0.3291 - val_loss: 2.2182\n",
      "Epoch 12/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.4166 - loss: 1.8990 - val_acc: 0.3376 - val_loss: 2.2164\n",
      "Epoch 13/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.4382 - loss: 1.8425 - val_acc: 0.3249 - val_loss: 2.2318\n",
      "Epoch 14/100\n",
      "67/67 - 2s - 36ms/step - acc: 0.4523 - loss: 1.7945 - val_acc: 0.3207 - val_loss: 2.2267\n",
      "Epoch 15/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.4523 - loss: 1.7501 - val_acc: 0.3502 - val_loss: 2.2210\n",
      "Epoch 16/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.4805 - loss: 1.6867 - val_acc: 0.3291 - val_loss: 2.2377\n",
      "Epoch 17/100\n",
      "67/67 - 3s - 41ms/step - acc: 0.4979 - loss: 1.6404 - val_acc: 0.3502 - val_loss: 2.2474\n",
      "Epoch 18/100\n",
      "67/67 - 2s - 35ms/step - acc: 0.5049 - loss: 1.5876 - val_acc: 0.3629 - val_loss: 2.2392\n",
      "Epoch 19/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.5373 - loss: 1.5213 - val_acc: 0.3418 - val_loss: 2.2608\n",
      "Epoch 20/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.5453 - loss: 1.4593 - val_acc: 0.3629 - val_loss: 2.2725\n",
      "Epoch 21/100\n",
      "67/67 - 3s - 40ms/step - acc: 0.5768 - loss: 1.3868 - val_acc: 0.3671 - val_loss: 2.2659\n",
      "Epoch 22/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.5918 - loss: 1.3321 - val_acc: 0.3797 - val_loss: 2.2990\n",
      "Epoch 23/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.6031 - loss: 1.2578 - val_acc: 0.3713 - val_loss: 2.2902\n",
      "Epoch 24/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.6209 - loss: 1.2088 - val_acc: 0.3629 - val_loss: 2.3336\n",
      "Epoch 25/100\n",
      "67/67 - 3s - 40ms/step - acc: 0.6501 - loss: 1.1466 - val_acc: 0.3586 - val_loss: 2.3402\n",
      "Epoch 26/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.6623 - loss: 1.0875 - val_acc: 0.3755 - val_loss: 2.3493\n",
      "Epoch 27/100\n",
      "67/67 - 2s - 35ms/step - acc: 0.6768 - loss: 1.0220 - val_acc: 0.3840 - val_loss: 2.3924\n",
      "Epoch 28/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.7027 - loss: 0.9607 - val_acc: 0.3797 - val_loss: 2.3916\n",
      "Epoch 29/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.7201 - loss: 0.9209 - val_acc: 0.3713 - val_loss: 2.4281\n",
      "Epoch 30/100\n",
      "67/67 - 2s - 23ms/step - acc: 0.7487 - loss: 0.8515 - val_acc: 0.3924 - val_loss: 2.4402\n",
      "Epoch 31/100\n",
      "67/67 - 3s - 42ms/step - acc: 0.7553 - loss: 0.8154 - val_acc: 0.3840 - val_loss: 2.4350\n",
      "Epoch 32/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.7614 - loss: 0.7673 - val_acc: 0.3840 - val_loss: 2.4813\n",
      "Epoch 33/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.7778 - loss: 0.7313 - val_acc: 0.4008 - val_loss: 2.4870\n",
      "Epoch 34/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.7994 - loss: 0.6890 - val_acc: 0.3755 - val_loss: 2.5386\n",
      "Epoch 35/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.8126 - loss: 0.6459 - val_acc: 0.3882 - val_loss: 2.5939\n",
      "Epoch 36/100\n",
      "67/67 - 3s - 41ms/step - acc: 0.8314 - loss: 0.6000 - val_acc: 0.3713 - val_loss: 2.6309\n",
      "Epoch 37/100\n",
      "67/67 - 2s - 34ms/step - acc: 0.8389 - loss: 0.5756 - val_acc: 0.3924 - val_loss: 2.6311\n",
      "Epoch 38/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.8563 - loss: 0.5393 - val_acc: 0.4008 - val_loss: 2.6591\n",
      "Epoch 39/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8633 - loss: 0.4992 - val_acc: 0.3882 - val_loss: 2.6935\n",
      "Epoch 40/100\n",
      "67/67 - 3s - 42ms/step - acc: 0.8666 - loss: 0.4961 - val_acc: 0.3671 - val_loss: 2.7394\n",
      "Epoch 41/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.8751 - loss: 0.4642 - val_acc: 0.3840 - val_loss: 2.7713\n",
      "Epoch 42/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.8727 - loss: 0.4555 - val_acc: 0.3882 - val_loss: 2.8338\n",
      "Epoch 43/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.8854 - loss: 0.4274 - val_acc: 0.3797 - val_loss: 2.8165\n",
      "Epoch 44/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9042 - loss: 0.3962 - val_acc: 0.3882 - val_loss: 2.8734\n",
      "Epoch 45/100\n",
      "67/67 - 2s - 37ms/step - acc: 0.8953 - loss: 0.4033 - val_acc: 0.3797 - val_loss: 2.8748\n",
      "Epoch 46/100\n",
      "67/67 - 3s - 40ms/step - acc: 0.9065 - loss: 0.3635 - val_acc: 0.3882 - val_loss: 2.9533\n",
      "Epoch 47/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9164 - loss: 0.3465 - val_acc: 0.3840 - val_loss: 2.9061\n",
      "Epoch 48/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9272 - loss: 0.3191 - val_acc: 0.3755 - val_loss: 2.9555\n",
      "Epoch 49/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9169 - loss: 0.3147 - val_acc: 0.3797 - val_loss: 2.9964\n",
      "Epoch 50/100\n",
      "67/67 - 2s - 37ms/step - acc: 0.9277 - loss: 0.2927 - val_acc: 0.3840 - val_loss: 3.0483\n",
      "Epoch 51/100\n",
      "67/67 - 3s - 41ms/step - acc: 0.9267 - loss: 0.2934 - val_acc: 0.4008 - val_loss: 2.9981\n",
      "Epoch 52/100\n",
      "67/67 - 2s - 29ms/step - acc: 0.9347 - loss: 0.2759 - val_acc: 0.3840 - val_loss: 3.0631\n",
      "Epoch 53/100\n",
      "67/67 - 2s - 34ms/step - acc: 0.9375 - loss: 0.2759 - val_acc: 0.3966 - val_loss: 3.0985\n",
      "Epoch 54/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.9342 - loss: 0.2654 - val_acc: 0.3966 - val_loss: 3.1217\n",
      "Epoch 55/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.9347 - loss: 0.2469 - val_acc: 0.3924 - val_loss: 3.1426\n",
      "Epoch 56/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9413 - loss: 0.2416 - val_acc: 0.3840 - val_loss: 3.1677\n",
      "Epoch 57/100\n",
      "67/67 - 2s - 36ms/step - acc: 0.9399 - loss: 0.2401 - val_acc: 0.3713 - val_loss: 3.1936\n",
      "Epoch 58/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9530 - loss: 0.2072 - val_acc: 0.3755 - val_loss: 3.2024\n",
      "Epoch 59/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9413 - loss: 0.2125 - val_acc: 0.3629 - val_loss: 3.2765\n",
      "Epoch 60/100\n",
      "67/67 - 3s - 40ms/step - acc: 0.9488 - loss: 0.2035 - val_acc: 0.3671 - val_loss: 3.2886\n",
      "Epoch 61/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9540 - loss: 0.1969 - val_acc: 0.3713 - val_loss: 3.2846\n",
      "Epoch 62/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9544 - loss: 0.1907 - val_acc: 0.3755 - val_loss: 3.3496\n",
      "Epoch 63/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9563 - loss: 0.1912 - val_acc: 0.3586 - val_loss: 3.3923\n",
      "Epoch 64/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9605 - loss: 0.1871 - val_acc: 0.3713 - val_loss: 3.4086\n",
      "Epoch 65/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9558 - loss: 0.1870 - val_acc: 0.3629 - val_loss: 3.4469\n",
      "Epoch 66/100\n",
      "67/67 - 2s - 37ms/step - acc: 0.9657 - loss: 0.1695 - val_acc: 0.3797 - val_loss: 3.4373\n",
      "Epoch 67/100\n",
      "67/67 - 2s - 36ms/step - acc: 0.9558 - loss: 0.1691 - val_acc: 0.3713 - val_loss: 3.4636\n",
      "Epoch 68/100\n",
      "67/67 - 3s - 41ms/step - acc: 0.9742 - loss: 0.1422 - val_acc: 0.3629 - val_loss: 3.5178\n",
      "Epoch 69/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.9587 - loss: 0.1737 - val_acc: 0.3544 - val_loss: 3.5949\n",
      "Epoch 70/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9587 - loss: 0.1655 - val_acc: 0.3629 - val_loss: 3.5387\n",
      "Epoch 71/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9652 - loss: 0.1507 - val_acc: 0.3671 - val_loss: 3.5441\n",
      "Epoch 72/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9652 - loss: 0.1516 - val_acc: 0.3502 - val_loss: 3.5768\n",
      "Epoch 73/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9704 - loss: 0.1382 - val_acc: 0.3629 - val_loss: 3.6156\n",
      "Epoch 74/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.9681 - loss: 0.1378 - val_acc: 0.3629 - val_loss: 3.6806\n",
      "Epoch 75/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9652 - loss: 0.1380 - val_acc: 0.3713 - val_loss: 3.6852\n",
      "Epoch 76/100\n",
      "67/67 - 2s - 24ms/step - acc: 0.9662 - loss: 0.1374 - val_acc: 0.3629 - val_loss: 3.7401\n",
      "Epoch 77/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9685 - loss: 0.1370 - val_acc: 0.3713 - val_loss: 3.7255\n",
      "Epoch 78/100\n",
      "67/67 - 2s - 25ms/step - acc: 0.9728 - loss: 0.1237 - val_acc: 0.3544 - val_loss: 3.7321\n",
      "Epoch 79/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9756 - loss: 0.1159 - val_acc: 0.3671 - val_loss: 3.7675\n",
      "Epoch 80/100\n",
      "67/67 - 2s - 31ms/step - acc: 0.9709 - loss: 0.1267 - val_acc: 0.3586 - val_loss: 3.7741\n",
      "Epoch 81/100\n",
      "67/67 - 3s - 39ms/step - acc: 0.9676 - loss: 0.1261 - val_acc: 0.3586 - val_loss: 3.7932\n",
      "Epoch 82/100\n",
      "67/67 - 3s - 50ms/step - acc: 0.9695 - loss: 0.1149 - val_acc: 0.3460 - val_loss: 3.7704\n",
      "Epoch 83/100\n",
      "67/67 - 4s - 63ms/step - acc: 0.9770 - loss: 0.1125 - val_acc: 0.3544 - val_loss: 3.7895\n",
      "Epoch 84/100\n",
      "67/67 - 2s - 31ms/step - acc: 0.9798 - loss: 0.1030 - val_acc: 0.3671 - val_loss: 3.7533\n",
      "Epoch 85/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9770 - loss: 0.1051 - val_acc: 0.3671 - val_loss: 3.8241\n",
      "Epoch 86/100\n",
      "67/67 - 3s - 38ms/step - acc: 0.9737 - loss: 0.1094 - val_acc: 0.3586 - val_loss: 3.8801\n",
      "Epoch 87/100\n",
      "67/67 - 2s - 35ms/step - acc: 0.9779 - loss: 0.1003 - val_acc: 0.3629 - val_loss: 3.9196\n",
      "Epoch 88/100\n",
      "67/67 - 2s - 30ms/step - acc: 0.9793 - loss: 0.0947 - val_acc: 0.3460 - val_loss: 3.9393\n",
      "Epoch 89/100\n",
      "67/67 - 2s - 28ms/step - acc: 0.9756 - loss: 0.1051 - val_acc: 0.3376 - val_loss: 3.9714\n",
      "Epoch 90/100\n",
      "67/67 - 2s - 26ms/step - acc: 0.9850 - loss: 0.0897 - val_acc: 0.3586 - val_loss: 3.9927\n",
      "Epoch 91/100\n",
      "67/67 - 2s - 29ms/step - acc: 0.9742 - loss: 0.0994 - val_acc: 0.3502 - val_loss: 3.9684\n",
      "Epoch 92/100\n",
      "67/67 - 2s - 31ms/step - acc: 0.9779 - loss: 0.1010 - val_acc: 0.3418 - val_loss: 3.9886\n",
      "Epoch 93/100\n",
      "67/67 - 3s - 51ms/step - acc: 0.9784 - loss: 0.0901 - val_acc: 0.3502 - val_loss: 4.0773\n",
      "Epoch 94/100\n",
      "67/67 - 4s - 57ms/step - acc: 0.9765 - loss: 0.1000 - val_acc: 0.3418 - val_loss: 4.0909\n",
      "Epoch 95/100\n",
      "67/67 - 3s - 51ms/step - acc: 0.9775 - loss: 0.0965 - val_acc: 0.3544 - val_loss: 4.0856\n",
      "Epoch 96/100\n",
      "67/67 - 2s - 32ms/step - acc: 0.9775 - loss: 0.0950 - val_acc: 0.3629 - val_loss: 4.1227\n",
      "Epoch 97/100\n",
      "67/67 - 2s - 27ms/step - acc: 0.9760 - loss: 0.0937 - val_acc: 0.3502 - val_loss: 4.1527\n",
      "Epoch 98/100\n",
      "67/67 - 2s - 35ms/step - acc: 0.9812 - loss: 0.0857 - val_acc: 0.3418 - val_loss: 4.1646\n",
      "Epoch 99/100\n",
      "67/67 - 3s - 41ms/step - acc: 0.9803 - loss: 0.0887 - val_acc: 0.3629 - val_loss: 4.0841\n",
      "Epoch 100/100\n",
      "67/67 - 2s - 36ms/step - acc: 0.9807 - loss: 0.0876 - val_acc: 0.3755 - val_loss: 4.1356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x723044bdc4d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "large armies offices and sent \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict character\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # get the index of the character with the highest probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += out_char\n",
    "    return in_text\n",
    "\n",
    "inp = 'Large armies of'\n",
    "print(len(inp))\n",
    "print(generate_seq(model, mapping, 50, inp.lower(), 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
